FROM nvcr.io/nvidia/clara/bionemo-framework:nightly

# Disable NVIDIA repos to prevent accidental upgrades.
# RUN cd /etc/apt/sources.list.d && \
#    mv cuda-ubuntu2404-x86_64.list cuda-ubuntu2404-x86_64.list.disabled

# See https://github.com/databricks/containers/blob/master/ubuntu/minimal/Dockerfile
RUN apt-get update && \
    apt-get install --yes \
      openjdk-8-jdk \
      iproute2 \
      bash \
      sudo \
      coreutils \
      procps \
      acl \
      wget && \
    /var/lib/dpkg/info/ca-certificates-java.postinst configure && \
    apt-get clean && \
    rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# install RStudio since command `R` is required for setting up driver on cluster creation
# See https://github.com/databricks/containers/blob/14042896b64285948300ed2d88a59eda87bb2a4d/ubuntu/R/Dockerfile#L16-L29
ENV DEBIAN_FRONTEND=noninteractive
RUN apt-get update \
  && apt-get install --yes software-properties-common apt-transport-https \
  && gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9 \
  && gpg -a --export E298A3A825C0D65DFD57CBB651716619E084DAB9 | sudo apt-key add - \
  && add-apt-repository -y "deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu $(lsb_release -cs)-cran40/" \
  && apt-get update \
  && apt-get install --yes \
    libssl-dev \
    r-base \
    r-base-dev \
  && add-apt-repository -r "deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu $(lsb_release -cs)-cran40/" \
  && apt-key del E298A3A825C0D65DFD57CBB651716619E084DAB9 \
  && apt-get clean \
  && rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/*

# Add new user for cluster library installation
RUN useradd libraries \
&& usermod -L libraries

ARG python_version="3.12"
ARG pip_version="24.0.0"
ARG setuptools_version="69.2.0"
ARG wheel_version="0.38.4"
ARG virtualenv_version="20.16.7"

WORKDIR /databricks

# Install python 3.12 from ubuntu.
# Install pip via get-pip.py bootstrap script and install versions that match Anaconda distribution.
# RUN apt-get update && apt-get upgrade -y \
#   && apt-get install -y curl software-properties-common \
#   && add-apt-repository ppa:deadsnakes/ppa -y \
#   && apt-get update \
#   && apt-get install -y python${python_version} python${python_version}-dev \
#   && curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py \
#   && /usr/bin/python${python_version} get-pip.py pip==${pip_version} setuptools==${setuptools_version} wheel==${wheel_version} \
#   && rm get-pip.py

# RUN apt-get update && apt-get install -y python${python_version}-venv \
#  && python${python_version} -m venv --system-site-packages --without-pip /databricks/python3 \
#   && curl -sS https://bootstrap.pypa.io/get-pip.py -o get-pip.py \
#   && /databricks/python3/bin/python${python_version} get-pip.py pip==${pip_version} setuptools==${setuptools_version} wheel==${wheel_version} \
#   && rm get-pip.py


# virtualenv 20.0.24 introduced a periodic update feature, which attempts to update all
# seeder packages every 14 days. This launches background processes that may interfere
# with user cleanup and may allow users to inadvertently update pip to newer versions
# incompatible with Databricks. Instead, we patch virtualenv to disable periodic updates per
# https://virtualenv.pypa.io/en/latest/user_guide.html#embed-wheels-for-distributions.
# RUN /usr/local/bin/pip${python_version} install --no-cache-dir virtualenv==${virtualenv_version} \
#     && sed -i -r 's/^(PERIODIC_UPDATE_ON_BY_DEFAULT) = True$/\1 = False/' /usr/local/lib/python${python_version}/dist-packages/virtualenv/seed/embed/base_embed.py \
#     && /usr/local/bin/pip${python_version} download pip==${pip_version} --dest \
#     /usr/local/lib/python${python_version}/dist-packages/virtualenv_support/


# Create /databricks/python3 environment.
# We install pip and wheel so their executables show up under /databricks/python3/bin.
# We use `--system-site-packages` so python will fallback to system site packages.
# We use `--no-download` so virtualenv will install the bundled pip and wheel.
# Initialize the default environment that Spark and notebooks will use
RUN python -m venv --system-site-packages /databricks/python3


# These python libraries are used by Databricks notebooks and the Python REPL
# You do not need to install pyspark - it is injected when the cluster is launched
# Versions are intended to reflect DBR 14.0: https://docs.databricks.com/release-notes/runtime/releases.html
# Certain libraries are added to avoiding breaking 15.4 LTS
COPY requirements.txt /databricks/.

RUN /databricks/python3/bin/pip install -r /databricks/requirements.txt

# Specifies where Spark will look for the python binary
ENV PYSPARK_PYTHON=/databricks/python3/bin/python3

RUN python -m venv --system-site-packages /databricks/python-lsp

COPY python-lsp-requirements.txt /databricks/.

RUN /databricks/python-lsp/bin/pip install -r /databricks/python-lsp-requirements.txt
